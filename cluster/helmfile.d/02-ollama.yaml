repositories:
  - name: ollama-helm
    url: https://otwld.github.io/ollama-helm/

releases:
  - name: ollama
    namespace: ollama
    chart: ollama-helm/ollama
    version: 1.11.0
#    version: 1.30.0
    values:
      - image:
          tag: 0.12.2
      - runtimeClassName: nvidia
      - nodeSelector:
          nvidia.com/gpu.memory: "4096" # TODO: Switch with 8192 / comfyui?
      - persistentVolume:
          enabled: true
          storageClass: nfs-csi
      - service:
          type: LoadBalancer
      - ollama:
          gpu:
            enabled: true
            type: nvidia
            number: 1
          models:
            # This didn't work, but manually created it within the pod and now it's on the NFS volume
            # create:
            #   - name: Qwen3-4B-Q4_K_M:latest
            #     template: |
            #       FROM /root/.ollama/Qwen3-4B-Instruct-2507-Q4_K_M.gguf
            pull:
              - phi4-mini
              - deepseek-r1:1.5b
              - qwen2.5:3b-instruct-q8_0
              - llama3.2:3b
            run:
              - qwen2.5:3b-instruct-q8_0